data:
  train_dir: ${env:PWD}/data/train
  test_dir: ${env:PWD}/data/test
  data_type: fcg
  batch_size: 16
  pin_memory: false
  num_workers: 6
  split_train_val: true
  split_ratios: [0.75, 0.25]
  consider_nodes: ${node_configuration.nodes}

model:
  convolution_count: 0

trainer:
  max_epochs: 100
  gpus: null

hydra:
  run:
    dir: output/${data.data_type}/${node_configuration.name}-conv_count=${model.convolution_count}
  sweep:
    dir: output/${data.data_type}
    subdir: ${node_configuration.name}-conv_count=${model.convolution_count}

defaults:
  - node_configuration: all
  - logger: wandb

# Additional arguments
# Any arguments to trainer will be passed to pytorch_lightning.Trainer()
# force_retrain=true will discard the previous stage and start training again
