import os
from typing import Tuple, Dict, List, Mapping

import dgl
import pytorch_lightning as pl
import pytorch_lightning.metrics.classification as metrics
import torch
import torch.nn.functional as F
from dgl.nn import Sequential, HeteroGraphConv, GraphConv
from pytorch_lightning.metrics import Metric
from torch import nn

package_directory = os.path.dirname(os.path.abspath(__file__))


class MalwareDetector(pl.LightningModule):
    def __init__(
            self,
            convolution_count: int,
            input_sizes: Dict[str, int],
            edges: List[Tuple[str, str, str]],
            all_metrics=True
    ):
        super().__init__()
        self.save_hyperparameters()
        # Edge -> Source mapping
        source_of = {x[1]: x[0] for x in edges}
        # Concrete types of nodes to be considered
        self.consider_nodes = list(input_sizes.keys())
        self.convolution_count = convolution_count
        self.convolution_layers = []
        hidden_dimensions = [32, 16, 8, 4]
        previous_dimension = 0
        if self.convolution_count > 0:
            input_layer = HeteroGraphConv({
                edge: GraphConv(input_sizes[source], hidden_dimensions[0], activation=F.relu)
                for edge, source in source_of.items()
            }, aggregate='sum')
            self.convolution_layers.append(input_layer)
            previous_dimension = hidden_dimensions[0]

        for dimension in hidden_dimensions[1:self.convolution_count]:
            layer = HeteroGraphConv({
                rel: GraphConv(previous_dimension, dimension, activation=F.relu)
                for rel in source_of
            }, aggregate='sum')
            self.convolution_layers.append(layer)
            previous_dimension = dimension

        self.convolution_layers = Sequential(*self.convolution_layers)
        # Latent vector is obtained by contacting all hidden representations
        self.input_sizes = input_sizes
        if self.convolution_count == 0:
            latent_dimension = sum(input_sizes.values())
        else:
            latent_dimension = len(self.consider_nodes) * previous_dimension
        self.last_dimension = previous_dimension
        self.classify = nn.Linear(latent_dimension, 1)
        # Metrics
        self.loss_func = nn.BCEWithLogitsLoss()
        self.all_metrics = all_metrics
        self.train_metrics = self._get_metric_dict('train', all_metrics)
        self.val_metrics = self._get_metric_dict('val', all_metrics)
        self.test_metrics = self._get_metric_dict('test', all_metrics)
        self.test_outputs = nn.ModuleDict({
            'confusion_matrix': metrics.ConfusionMatrix(num_classes=2),
            'prc': metrics.PrecisionRecallCurve(compute_on_step=False),
            'roc': metrics.ROC(compute_on_step=False)
        })

    @staticmethod
    def _get_metric_dict(stage: str, all_metrics: bool) -> Mapping[str, Metric]:
        metric_dict = {f'{stage}_accuracy': metrics.Accuracy()}
        if all_metrics:
            print("Got all metrics = True")
            metric_dict = {
                **metric_dict,
                f'{stage}_precision': metrics.Precision(num_classes=1),
                f'{stage}_recall': metrics.Recall(num_classes=1),
                f'{stage}_f1': metrics.FBeta(num_classes=1)
            }
        return nn.ModuleDict(metric_dict)

    def get_latent_vector(self, g: dgl.DGLHeteroGraph) -> torch.Tensor:
        # Dont change the parameter in forward
        # with g.local_scope():
        if len(self.consider_nodes) > 1:
            h = g.ndata['features']
        else:
            h = {self.consider_nodes[0]: g.ndata['features']}
        h = self.convolution_layers(g, h)
        features = []
        if len(self.consider_nodes) == 1:
            node_name = self.consider_nodes[0]
            g.ndata['h'] = h[node_name] if len(self.convolution_layers) > 0 else h[0][node_name]
            features.append(dgl.mean_nodes(g, 'h'))
        else:
            g.ndata['h'] = h if len(self.convolution_layers) > 0 else h[0]
            for node_type in self.consider_nodes:
                if node_type in g.ndata['h'].keys():
                    features.append(dgl.mean_nodes(g, 'h', ntype=node_type))
                else:
                    features.append(
                        torch.zeros(
                            g.batch_size,
                            self.last_dimension if len(self.convolution_layers) > 0 else self.input_sizes[node_type],
                            device=self.device
                        )
                    )
        return torch.cat(features, dim=1)

    def forward(self, g: dgl.DGLHeteroGraph) -> torch.Tensor:
        return self.classify(self.get_latent_vector(g)).squeeze()

    def training_step(self, batch: Tuple[dgl.DGLGraph, torch.Tensor], batch_idx: int) -> torch.Tensor:
        bg, label = batch
        logits = self.forward(bg)
        loss = self.loss_func(logits, label)
        prediction = torch.sigmoid(logits)
        for metric_name, metric in self.train_metrics.items():
            metric.update(prediction, label)
        self.log('train_loss', loss, on_step=True, on_epoch=True)
        return loss

    def validation_step(self, batch: Tuple[dgl.DGLGraph, torch.Tensor], batch_idx: int):
        bg, label = batch
        logits = self.forward(bg)
        loss = self.loss_func(logits, label)
        prediction = torch.sigmoid(logits)
        for metric_name, metric in self.val_metrics.items():
            metric.update(prediction, label)
        self.log('val_loss', loss, on_step=False, on_epoch=True)
        return loss

    def test_step(self, batch: Tuple[dgl.DGLGraph, torch.Tensor], batch_idx: int):
        bg, label = batch
        logits = self.forward(bg)
        prediction = torch.sigmoid(logits)
        loss = self.loss_func(logits, label)
        for metric_name, metric in self.test_metrics.items():
            metric.update(prediction, label)
        for metric_name, metric in self.test_outputs.items():
            metric.update(prediction, label)
        self.log('test_loss', loss, on_step=False, on_epoch=True)
        return loss

    def configure_optimizers(self) -> torch.optim.Adam:
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)
        return optimizer
