import os
from typing import Tuple, Dict, List

import dgl
import pytorch_lightning as pl
import pytorch_lightning.metrics.classification as metrics
import torch
import torch.nn.functional as F
from dgl.nn import Sequential, HeteroGraphConv, GraphConv
from torch import nn

package_directory = os.path.dirname(os.path.abspath(__file__))


class MalwareDetector(pl.LightningModule):
    def __init__(
            self,
            convolution_count: int,
            input_sizes: Dict[str, int],
            edges: List[Tuple[str, str, str]]
    ):
        super().__init__()
        self.save_hyperparameters()
        # Edge -> Source mapping
        source_of = {x[1]: x[0] for x in edges}
        # Concrete types of nodes to be considered
        self.consider_nodes = list(input_sizes.keys())
        self.convolution_count = convolution_count
        self.convolution_layers = []
        hidden_dimensions = [32, 16, 8, 4]
        # In case of convolution_count == 0, we concat the
        # means of features of all nodes, and pass it to
        # Linear layer
        previous_dimension = 0
        if self.convolution_count > 0:
            input_layer = HeteroGraphConv({
                edge: GraphConv(input_sizes[source], hidden_dimensions[0], activation=F.relu)
                for edge, source in source_of.items()
            }, aggregate='sum')
            self.convolution_layers.append(input_layer)
            previous_dimension = hidden_dimensions[0]

        for dimension in hidden_dimensions[1:self.convolution_count]:
            layer = HeteroGraphConv({
                rel: GraphConv(previous_dimension, dimension, activation=F.relu)
                for rel in source_of
            }, aggregate='sum')
            self.convolution_layers.append(layer)
            previous_dimension = dimension

        self.convolution_layers = Sequential(*self.convolution_layers)
        if self.convolution_count == 0:
            latent_dimension = sum(input_sizes.values())
        else:
            latent_dimension = len(self.consider_nodes) * previous_dimension
        self.classify = nn.Linear(latent_dimension, 1)
        # Metrics
        self.loss_func = nn.BCEWithLogitsLoss()
        self.loggable_metrics = nn.ModuleDict({
            'accuracy': metrics.Accuracy(),
            'precision': metrics.Precision(num_classes=1),
            'recall': metrics.Recall(num_classes=1),
            'f1': metrics.FBeta(num_classes=1)
        })
        self.non_loggable_metrics = nn.ModuleDict({
            'confusion_matrix': metrics.ConfusionMatrix(num_classes=2),
            'pr_curve': metrics.PrecisionRecallCurve(compute_on_step=False),
            'roc': metrics.ROC(compute_on_step=False)
        })

    def get_latent_vector(self, g: dgl.DGLHeteroGraph) -> torch.Tensor:
        # Dont change the parameter in forward
        # with g.local_scope():
        if len(self.consider_nodes) > 1:
            h = g.ndata['features']
        else:
            h = {self.consider_nodes[0]: g.ndata['features']}
        h = self.convolution_layers(g, h)
        features = []
        if len(self.consider_nodes) == 1:
            node_name = self.consider_nodes[0]
            g.ndata['h'] = h[node_name] if len(self.convolution_layers) > 0 else h[0][node_name]
            features.append(dgl.mean_nodes(g, 'h'))
        else:
            g.ndata['h'] = h if len(self.convolution_layers) > 0 else h[0]
            for node_type in g.ndata['h'].keys():
                features.append(dgl.mean_nodes(g, 'h', ntype=node_type))
        return torch.cat(features, dim=1)

    def forward(self, g: dgl.DGLHeteroGraph) -> torch.Tensor:
        return self.classify(self.get_latent_vector(g)).squeeze()

    def training_step(self, batch: Tuple[dgl.DGLGraph, torch.Tensor], batch_idx: int) -> torch.Tensor:
        bg, label = batch
        logits = self.forward(bg)
        loss = self.loss_func(logits, label)
        prediction = torch.sigmoid(logits)
        accuracy = self.loggable_metrics['accuracy'](prediction, label)
        self.log('train_loss', loss, on_step=True, on_epoch=True)
        self.log('train_accuracy', accuracy, on_step=True, on_epoch=True)
        return loss

    def validation_step(self, batch: Tuple[dgl.DGLGraph, torch.Tensor], batch_idx: int):
        bg, label = batch
        logits = self.forward(bg)
        loss = self.loss_func(logits, label)
        prediction = torch.sigmoid(logits)
        accuracy = self.loggable_metrics['accuracy'](prediction, label)
        self.log('val_loss', loss, on_step=False, on_epoch=True)
        self.log('val_accuracy', accuracy, on_step=False, on_epoch=True)

    def test_step(self, batch: Tuple[dgl.DGLGraph, torch.Tensor], batch_idx: int):
        bg, label = batch
        logits = self.forward(bg)
        prediction = torch.sigmoid(logits)
        test_metrics = {
            'test_loss': self.loss_func(logits, label),
        }
        for metric_name, metric in self.loggable_metrics.items():
            test_metrics[f'test_{metric_name}'] = metric(prediction, label)
        for metric_name, metric in self.non_loggable_metrics.items():
            # Update metrics
            metric(prediction, label)
        self.log_dict(test_metrics, on_step=False, on_epoch=True)

    def configure_optimizers(self) -> torch.optim.Adam:
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)
        return optimizer
