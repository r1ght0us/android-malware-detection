import os
from pathlib import Path
from typing import Tuple

import dgl
import pytorch_lightning as pl
import pytorch_lightning.metrics.classification as metrics
import torch
from dgl.nn import Sequential, HeteroGraphConv, GraphConv
from torch import nn
import torch.nn.functional as F

from scripts.process_dataset import get_api_list, APIConfig

package_directory = os.path.dirname(os.path.abspath(__file__))


class MalwareDetector(pl.LightningModule):
    def __init__(
            self,
            convolution_count=0,
            data_type='fcg',
            consider_nodes=['user', 'api', 'permission']
    ):
        """
        Create MalwareDetector module
        :param convolution_count: Number of convolution layer
        :param data_type: Either FCG or Reduced FCG (RFCG)
        :param consider_nodes: List of nodes to be considered
        """
        super().__init__()
        # Load aux files
        self.api_list_length = len(get_api_list())
        self.permissions_count = len(APIConfig(Path(package_directory).parent / "metadata").permission_to_int)
        # Checks
        if len(consider_nodes) == 0:
            raise ValueError(f'No nodes to consider!')
        # Node generic type to node concrete type mapping
        self.node_types = {
            'user': 'method' if data_type == 'fcg' else 'class',
            'api': 'api_method' if data_type == 'fcg' else 'api_class',
            'permission': 'permission'
        }
        # Node type to input dimension mapping
        input_dimensions = {
            x: y for x, y in {
                'user': 21,  # 21 different opcode groups
                'api': self.api_list_length,  # Possible number of packages, 226
                'permission': self.permissions_count,  # Possible number of permissions, 177
            }.items() if x in consider_nodes
        }
        # Possible edges that can be encountered in inputs of current module
        edges = [
            x for x in [
                ('user', 'invokes', 'user'),
                ('user', 'invokes_api', 'api'),
                ('api', 'callback', 'api'),
                ('user', 'parent_of', 'user'),  # override by
                ('api', 'parent_of_user', 'user'),
                ('api', 'parent_of_api', 'api'),
                ('permission', 'used_by', 'api')
            ]
            if x[0] in consider_nodes and x[2] in consider_nodes
        ]
        # Edge -> Source mapping, used to
        source_of = {x[1]: x[0] for x in edges}
        # Concrete types of nodes to be considered
        self.consider_nodes = [self.node_types[x] for x in consider_nodes]
        self.convolution_count = convolution_count
        self.convolution_layers = []
        hidden_dimensions = [128, 64, 32, 16]
        # In case of convolution_count == 0, we concat the
        # means of features of all nodes, and pass it to
        # Linear layer
        previous_dimension = sum(input_dimensions.values())
        if self.convolution_count > 0:
            input_layer = HeteroGraphConv({
                edge: GraphConv(input_dimensions[source], hidden_dimensions[0], norm='none', activation=F.relu)
                for edge, source in source_of.items()
            }, aggregate='sum')
            self.convolution_layers.append(input_layer)
            previous_dimension = hidden_dimensions[0]

        for dimension in hidden_dimensions[1:self.convolution_count]:
            layer = HeteroGraphConv({
                rel: GraphConv(previous_dimension, dimension, norm='none', activation=F.relu)
                for rel in source_of
            }, aggregate='sum')
            self.convolution_layers.append(layer)
            previous_dimension = dimension

        self.convolution_layers = Sequential(*self.convolution_layers)
        self.last_dimension = previous_dimension
        self.classify = nn.Linear(previous_dimension, 1)
        self.loss_func = nn.BCEWithLogitsLoss()
        # Metrics
        self.loggable_metrics = nn.ModuleDict({
            'accuracy': metrics.Accuracy(),
            'precision': metrics.Precision(num_classes=1),
            'recall': metrics.Recall(num_classes=1),
            'f1': metrics.FBeta(num_classes=1)
        })
        self.non_loggable_metrics = nn.ModuleDict({
            'confusion_matrix': metrics.ConfusionMatrix(num_classes=1),
            'pr_curve': metrics.PrecisionRecallCurve(num_classes=1),
            'roc': metrics.ROC(num_classes=1)
        })

    def _process_api_package(self, g: dgl.DGLHeteroGraph) -> dgl.DGLHeteroGraph:
        api_node_name = [x for x in g.ntypes if "api" in x][0]
        nodes = torch.arange(len(g.nodes(api_node_name)), device=self.device)
        api_packages = g.ndata['features'][api_node_name].long()
        dim_0 = nodes[torch.where(api_packages >= 0)].view(-1, 1)
        dim_1 = api_packages[torch.where(api_packages >= 0)].view(-1, 1)
        indices = torch.cat([dim_0, dim_1], dim=1).t()
        values = torch.ones(len(dim_0), device=self.device)
        size = torch.Size([len(g.nodes(api_node_name)), self.api_list_length])
        g.ndata['features'] = {api_node_name: torch.sparse.ShortTensor(indices, values, size).to_dense().squeeze()}
        return g

    def _process_permissions(self, g: dgl.DGLHeteroGraph) -> dgl.DGLHeteroGraph:
        nodes = torch.arange(len(g.nodes('permission')), device=self.device)
        permissions = g.ndata['features']['permission'].long()
        dim_0 = nodes[torch.where(permissions < self.permissions_count)].view(-1, 1)
        dim_1 = permissions[torch.where(permissions < self.permissions_count)].view(-1, 1)
        indices = torch.cat([dim_0, dim_1], dim=1).t()
        values = torch.ones(len(dim_0), device=self.device)
        size = torch.Size([len(g.nodes('permission')), self.permissions_count])
        g.ndata['features'] = {'permission': torch.sparse.ShortTensor(indices, values, size).to_dense().squeeze()}
        return g

    def _convert_to_float(self, g: dgl.DGLHeteroGraph) -> dgl.DGLHeteroGraph:
        result = {}
        for node_type in g.ntypes:
            result[node_type] = g.ndata['features'][node_type].float()
        g.ndata['features'] = result
        return g

    def forward(self, g: dgl.DGLHeteroGraph) -> torch.Tensor:
        # Dont change the parameter in forward
        with g.local_scope():
            g = self._process_api_package(g)
            g = self._process_permissions(g)
            g = self._convert_to_float(g)
            if g.batch_size > 1:
                graphs = dgl.unbatch(g)
                for i in range(len(graphs)):
                    graphs[i] = dgl.node_type_subgraph(graphs[i], self.consider_nodes)
                g = dgl.batch(graphs)
            else:
                g = dgl.node_type_subgraph(g, self.consider_nodes)
            if len(self.consider_nodes) > 1:
                h = g.ndata['features']
            else:
                h = {self.consider_nodes[0]: g.ndata['features']}
            h = self.convolution_layers(g, h)
            if len(self.consider_nodes) == 1:
                node_name = self.consider_nodes[0]
                g.ndata['h'] = h[node_name] if len(self.convolution_layers) > 0 else h[0][node_name]
            else:
                g.ndata['h'] = h if len(self.convolution_layers) > 0 else h[0]
            hg = 0
            if self.convolution_count == 0:
                features = []
                if self.node_types['api'] in g.ntypes:
                    feature = dgl.mean_nodes(g, 'h', ntype=self.node_types['api'])
                    features.append(feature)
                if self.node_types['user'] in g.ntypes:
                    feature = dgl.mean_nodes(g, 'h', ntype=self.node_types['user'])
                    features.append(feature)
                if self.node_types['permission'] in g.ntypes:
                    feature = dgl.mean_nodes(g, 'h', ntype='permission')
                    features.append(feature)
                hg = torch.cat(features, dim=1)
            else:
                # Features from all nodes have same dimension now
                for node_type in set(g.ntypes) - {'permission'}:
                    hg = hg + dgl.mean_nodes(g, 'h', ntype=node_type)
            return self.classify(hg).squeeze()

    def training_step(self, batch: Tuple[dgl.DGLGraph, torch.Tensor], batch_idx: int) -> torch.Tensor:
        bg, label = batch
        prediction = self.forward(bg)
        loss = self.loss_func(prediction, label)
        accuracy = self.loggable_metrics['accuracy'](prediction, label)
        self.log('train_loss', loss, on_step=True, on_epoch=True)
        self.log('train_accuracy', accuracy, on_epoch=True)
        return loss

    def validation_step(self, batch: Tuple[dgl.DGLGraph, torch.Tensor], batch_idx: int):
        bg, label = batch
        prediction = self.forward(bg)
        loss = self.loss_func(prediction, label)
        accuracy = self.loggable_metrics['accuracy'](prediction, label)
        self.log('val_loss', loss, on_step=False, on_epoch=True)
        self.log('val_accuracy', accuracy, on_epoch=True)

    def test_step(self, batch: Tuple[dgl.DGLGraph, torch.Tensor], batch_idx: int):
        bg, label = batch
        prediction = self.forward(bg)
        logits = torch.sigmoid(prediction)
        test_metrics = {
            'test_loss': self.loss_func(prediction, label),
        }
        for metric_name, metric in self.loggable_metrics.items():
            test_metrics[f'test_{metric_name}'] = metric(logits, label)
        for metric_name, metric in self.loggable_metrics.items():
            # Update metrics
            metric(logits, label)
        self.log_dict(test_metrics, on_step=False, on_epoch=True)

    def configure_optimizers(self) -> torch.optim.Adam:
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3, amsgrad=True)
        return optimizer
