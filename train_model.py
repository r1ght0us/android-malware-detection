import os
from pathlib import Path

import hydra
from omegaconf import DictConfig
from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.loggers import WandbLogger

from core.callbacks import InputMonitor
from core.data_module import MalwareDataModule
from core.model import MalwareDetector


@hydra.main(config_path="config", config_name="conf")
def train_model(cfg: DictConfig) -> None:
    data_module = MalwareDataModule(**cfg['data'], consider_nodes=cfg['node_configuration']['nodes'])

    model_kwargs = {
        **cfg['model'],
        'input_sizes': data_module.input_sizes,
        'edges': data_module.edges
    }
    model = MalwareDetector(**model_kwargs)

    callbacks = [ModelCheckpoint(
        dirpath=os.getcwd(),
        filename=str('{epoch:02d}-{val_loss:.2f}.pt'),
        monitor='val_loss',
        mode='min',
        save_last=True,
        save_top_k=-1
    )]
    trainer_kwargs = dict(cfg['trainer'])
    force_retrain = cfg.get('force_retrain', False)
    if Path('last.ckpt').exists() and not force_retrain:
        trainer_kwargs['resume_from_checkpoint'] = 'last.ckpt'

    if 'logger' in cfg:
        # We use WandB logger
        logger = WandbLogger(
            **cfg['logger']['args'],
            job_type=f'testing_{cfg["testing"]}' if "testing" in cfg else "training"
        )
        logger.watch(model)
        logger.log_hyperparams(cfg['logger']['hparams'])
        if logger:
            trainer_kwargs['logger'] = logger
            callbacks.append(InputMonitor())
    trainer = Trainer(
        callbacks=callbacks,
        **trainer_kwargs
    )
    testing = cfg.get('testing', '')
    if not testing:
        trainer.fit(model, datamodule=data_module)
    else:
        if testing != 'best' and testing != 'last' and 'epoch' not in testing:
            raise ValueError(f"testing must be one of 'best' or 'last' or 'epoch=N'. It is {testing}")
        if testing == 'best':
            ckpt_path = 'best'
        elif testing == 'last':
            ckpt_path = 'last.ckpt'
        else:
            # epoch in testing
            epoch = testing.split('@')[1]
            checkpoints = list(Path(os.getcwd()).glob(f"epoch={epoch}*.ckpt"))
            if len(checkpoints) < 0:
                print(f"Checkpoint at epoch = {epoch} not found.")
            assert len(checkpoints) == 1, f"Multiple checkpoints corresponding to epoch = {epoch} found."
            ckpt_path = checkpoints[0]
        print(f"Using checkpoint {ckpt_path} for testing.")
        model = MalwareDetector.load_from_checkpoint(ckpt_path, **model_kwargs)
        trainer.test(model, datamodule=data_module, verbose=True)


if __name__ == '__main__':
    train_model()
