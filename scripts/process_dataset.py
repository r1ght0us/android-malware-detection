import argparse
import functools
import json
import multiprocessing
import os
import sys
import traceback
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Union, Tuple, Set, Callable, Any, Optional

import dgl
import joblib as J
import networkx as nx
import torch
from androguard.core.analysis.analysis import MethodAnalysis, Analysis, ClassAnalysis
from androguard.core.api_specific_resources import load_permission_mappings
from androguard.misc import AnalyzeAPK
from pygtrie import StringTrie

from process_api import get_inheritance_graph

package_directory = os.path.dirname(os.path.abspath(__file__))

stats: Dict[str, int] = defaultdict(int)


def memoize(function):
    """
    Alternative to @lru_cache which could not be pickled in ray
    :param function: Function to be cached
    :return: Wrapped function
    """
    memo = {}

    def wrapper(*args):
        if args in memo:
            return memo[args]
        else:
            rv = function(*args)
            memo[args] = rv
            return rv

    return wrapper


class APIConfig:
    def __init__(self, metadata_dir: Union[str, Path]):
        metadata_dir = Path(metadata_dir)
        if not metadata_dir.exists():
            raise FileNotFoundError(f"{metadata_dir} doesn't exist")
        self.result = json.load(open(metadata_dir / "api_analysis/final_callbacks.json"))
        self.access_flags = json.load(open(metadata_dir / "api_analysis/access_flags.json"))
        self.method_names = json.load(open(metadata_dir / "api_analysis/method_names.json"))
        self.ig = nx.readwrite.json_graph.node_link_graph(
            json.load(open(metadata_dir / "api_analysis/inheritance_graph.json")))
        self.ag = nx.readwrite.json_graph.node_link_graph(
            json.load(open(metadata_dir / "api_analysis/argument_graph.json")))
        self.result_remapping = self._remap_result()
        self.registration_methods = set()
        self.permission_mapping = load_permission_mappings(apilevel=25)
        self.permission_to_int = {
            x: i for i, x in
            enumerate(sorted(set(sum(self.permission_mapping.values(), []))))
            if x.startswith("android.permission")
        }
        for x in self.result:
            for used_class in x['used_classes']:
                class_ = x['used_classes'][used_class]
                self.registration_methods.update(class_['registration_methods'])

    def _remap_result(self):
        result_remapping = {}
        result = list(self.result)
        for entry in result:
            callback_class = entry['callback_class']
            used_classes = entry['used_classes']
            for used_class in used_classes:
                registration_methods = used_classes[used_class]['registration_methods']
                mapping = used_classes[used_class]['used->callback mapping']
                if used_class not in result_remapping:
                    result_remapping[used_class] = {
                        'registration_methods': set(registration_methods),
                        'used->callback mapping': set(map(tuple, mapping)),
                        'callback_classes': {callback_class}
                    }
                else:
                    (result_remapping[used_class]['registration_methods']).update(registration_methods)
                    (result_remapping[used_class]['used->callback mapping']).update(map(tuple, mapping))
                    (result_remapping[used_class]['callback_classes']).add(callback_class)
        return result_remapping


class FeatureExtractors:
    NUM_PERMISSION_GROUPS = 20
    NUM_API_PACKAGES = 226
    PERMISSION_MAPPINGS = load_permission_mappings(apilevel=25)

    @staticmethod
    @memoize
    def get_used_permissions(api: MethodAnalysis) -> List[str]:
        if not api.is_external():
            return []
        return FeatureExtractors.PERMISSION_MAPPINGS.get(
            # This is their format!
            f'{api.class_name}-{api.name}-{api.descriptor}',
            []
        )

    @staticmethod
    def _get_opcode_mapping() -> Dict[str, int]:
        """
        Group opcodes and assign them an ID
        :return: Mapping from opcode group name to their ID
        """
        mapping = {x: i for i, x in enumerate(['nop', 'mov', 'return',
                                               'const', 'monitor', 'check-cast', 'instanceof', 'new',
                                               'fill', 'throw', 'goto/switch', 'cmp', 'if', 'unused',
                                               'arrayop', 'instanceop', 'staticop', 'invoke',
                                               'unaryop', 'binop', 'inline'])}
        mapping['invalid'] = -1
        return mapping

    @staticmethod
    @memoize
    def _get_instruction_type(op_value: int) -> str:
        """
        Get instruction group name from instruction
        :param op_value: Opcode value
        :return: String containing ID of :instr:
        """
        if 0x00 == op_value:
            return 'nop'
        elif 0x01 <= op_value <= 0x0D:
            return 'mov'
        elif 0x0E <= op_value <= 0x11:
            return 'return'
        elif 0x12 <= op_value <= 0x1C:
            return 'const'
        elif 0x1D <= op_value <= 0x1E:
            return 'monitor'
        elif 0x1F == op_value:
            return 'check-cast'
        elif 0x20 == op_value:
            return 'instanceof'
        elif 0x22 <= op_value <= 0x23:
            return 'new'
        elif 0x24 <= op_value <= 0x26:
            return 'fill'
        elif 0x27 == op_value:
            return 'throw'
        elif 0x28 <= op_value <= 0x2C:
            return 'goto/switch'
        elif 0x2D <= op_value <= 0x31:
            return 'cmp'
        elif 0x32 <= op_value <= 0x3D:
            return 'if'
        elif (0x3E <= op_value <= 0x43) or (op_value == 0x73) or (0x79 <= op_value <= 0x7A) or (
                0xE3 <= op_value <= 0xED):
            return 'unused'
        elif (0x44 <= op_value <= 0x51) or (op_value == 0x21):
            return 'arrayop'
        elif (0x52 <= op_value <= 0x5F) or (0xF2 <= op_value <= 0xF7):
            return 'instanceop'
        elif 0x60 <= op_value <= 0x6D:
            return 'staticop'
        elif (0x6E <= op_value <= 0x72) or (0x74 <= op_value <= 0x78) or (0xF0 == op_value) or (
                0xF8 <= op_value <= 0xFB):
            return 'invoke'
        elif 0x7B <= op_value <= 0x8F:
            return 'unaryop'
        elif 0x90 <= op_value <= 0xE2:
            return 'binop'
        elif 0xEE == op_value:
            return 'inline'
        else:
            return 'invalid'

    @staticmethod
    def _mapping_to_bitstring(mapping: List[int], max_len) -> torch.Tensor:
        """
        Convert opcode mappings to bitstring
        :param max_len:
        :param mapping: List of IDs of opcode groups (present in an method)
        :return: Binary tensor of length `len(opcode_mapping)` with value 1 at positions specified by :poram mapping:
        """
        size = torch.Size([1, max_len - 1])
        if len(mapping) > 0:
            indices = torch.LongTensor([[0, x] for x in mapping]).t()
            values = torch.LongTensor([1] * len(mapping))
            tensor = torch.sparse.LongTensor(indices, values, size)
        else:
            tensor = torch.sparse.LongTensor(size)
        # Sparse tensor is normal tensor on CPU!
        return tensor.to_dense().squeeze()

    @staticmethod
    def _get_api_trie() -> StringTrie:
        apis = open(Path(package_directory).parent / "metadata" / "api.list").readlines()
        api_list = {x.strip(): i for i, x in enumerate(apis)}
        api_trie = StringTrie(separator='.')
        for k, v in api_list.items():
            api_trie[k] = v
        return api_trie

    @staticmethod
    def _get_permission_mapping() -> Dict[str, Tuple[int, int]]:
        lines = open(Path(package_directory).parent / "metadata" / "permissions.list").readlines()
        permission_mapping = {}
        for line in lines:
            permission, dangerous, group = line.strip().split('\t')
            permission_mapping[permission] = (int(dangerous), int(group))
        return permission_mapping

    @staticmethod
    @memoize
    def get_api_features(api: Union[MethodAnalysis, ClassAnalysis]) -> Optional[torch.Tensor]:
        if not api.is_external():
            return None
        api_trie = FeatureExtractors._get_api_trie()
        if isinstance(api, ClassAnalysis):
            name = api.name
        else:
            name = api.class_name
        name = str(name)[1:-1].replace('/', '.')
        _, index = api_trie.longest_prefix(name)
        if index is None:
            indices = []
        else:
            indices = [index]
        feature_vector = FeatureExtractors._mapping_to_bitstring(indices, FeatureExtractors.NUM_API_PACKAGES)
        return feature_vector

    @staticmethod
    @memoize
    def get_user_features(user: Union[MethodAnalysis, ClassAnalysis]) -> Optional[torch.Tensor]:
        if user.is_external():
            return None
        opcode_mapping = FeatureExtractors._get_opcode_mapping()
        if isinstance(user, MethodAnalysis):
            opcode_groups = set()
            for instr in user.get_method().get_instructions():
                instruction_type = FeatureExtractors._get_instruction_type(instr.get_op_value())
                instruction_id = opcode_mapping[instruction_type]
                if instruction_id >= 0:
                    opcode_groups.add(instruction_id)
            feature_vector = FeatureExtractors._mapping_to_bitstring(list(opcode_groups), len(opcode_mapping))
            return torch.LongTensor(feature_vector)
        else:
            method_features = []
            for method in user.get_methods():
                method_feature = FeatureExtractors.get_user_features(method)
                if method_feature is not None:
                    method_features.append(method_feature)
            # Binary or = |
            base = torch.zeros(len(opcode_mapping) - 1, dtype=torch.int64)
            return functools.reduce(lambda x, y: x | y, method_features, base)

    @staticmethod
    @memoize
    def get_permission_features(permission: str) -> Optional[torch.Tensor]:
        permission_mapping = FeatureExtractors._get_permission_mapping()
        if permission not in permission_mapping:
            return None
        dangerous, group = permission_mapping[permission]
        feature_vector = FeatureExtractors._mapping_to_bitstring([group], FeatureExtractors.NUM_PERMISSION_GROUPS + 1)
        feature_vector[-1] = dangerous
        return feature_vector


class HeteroGraph:
    def __init__(
            self,
            edge_types: List[Tuple[str, str, str]],
            reverse: bool,
            feature_extractors: Dict[str, Callable[[Any], Optional[torch.Tensor]]],
            data_type_checks: Dict[str, Callable[[Any], bool]],

    ):
        self.edge_types = []
        self.node_names = set()
        edge_names = []
        self.reverse = reverse
        for edge_type in edge_types:
            src, edge_name, dest = edge_type
            self.node_names.add(src)
            self.node_names.add(dest)
            if edge_name in edge_names:
                raise ValueError(f"Duplicate edge name - {edge_name}")
            if self.reverse:
                reverse_edge = self.get_reverse_edge(edge_type)
                reverse_edge_name = reverse_edge[1]
                edge_names.append(reverse_edge_name)
                self.edge_types.append(reverse_edge)
            edge_names.append(edge_name)
            self.edge_types.append(edge_type)
        assert self.node_names == set(feature_extractors), "node_names!=set(feature_extractors)"
        assert self.node_names == set(data_type_checks), "node_names!=set(data_types)"
        self.feature_extractors = feature_extractors
        self.data_type_checks = data_type_checks
        self._node_to_int: Dict[str, Dict[Any, int]] = {
            node_name: {} for node_name in self.node_names
        }
        self._node_features: Dict[str, Dict[int, torch.Tensor]] = {
            node_name: {} for node_name in self.node_names
        }
        self._nodes: Dict[str, Set[int]] = {
            node_name: set() for node_name in self.node_names
        }
        self._edges: Dict[Tuple[str, str, str], Set[Tuple[int, int]]] = {
            edge_name: set() for edge_name in self.edge_types
        }

    @staticmethod
    def get_reverse_edge(edge):
        src, edge_name, dest = edge
        return dest, f'rev_{edge_name}', src

    def node_to_int(self, node: Any, node_type: str):
        node_mappings = self._node_to_int[node_type]
        if node not in node_mappings:
            node_mappings[node] = len(node_mappings)
        return node_mappings[node]

    def insert(self, source: Any, dest: Any, edge_type: Tuple[str, str, str]) -> bool:
        source_type, edge_name, dest_type = edge_type
        if edge_type not in self.edge_types:
            raise ValueError(f"Edge type {edge_type} not in self")
        assert self.data_type_checks[source_type](
            source), f"Data type check for {source_type} failed. Got {type(source)}"
        assert self.data_type_checks[dest_type](dest), f"Data type check for {dest_type} failed. Got {type(dest)}"
        source_id = self.node_to_int(source, source_type)
        dest_id = self.node_to_int(dest, dest_type)
        source_features = self.feature_extractors[source_type](source)
        dest_features = self.feature_extractors[dest_type](dest)
        if source_features is None or dest_features is None:
            stats["not source_features and dest_features"] += 1
            return False
        # Add nodes
        self._nodes[source_type].add(source_id)
        self._nodes[dest_type].add(dest_id)
        # Add node features
        self._node_features[source_type][source_id] = source_features
        self._node_features[dest_type][dest_id] = dest_features
        # Add edges
        self._edges[edge_type].add((source_id, dest_id))
        if self.reverse:
            reverse_edge_type = self.get_reverse_edge(edge_type)
            self._edges[reverse_edge_type].add((dest_id, source_id))
        return True

    def to_dgl(self) -> dgl.DGLHeteroGraph:
        # Checks
        num_nodes = {x: len(self._nodes[x]) for x in self.node_names}
        num_edges = {x: len(self._edges[x]) for x in self.edge_types}
        num_mappings = {x: len(self._node_to_int[x]) for x in self.node_names}
        num_features = {x: len(self._node_features[x]) for x in self.node_names}
        assert num_nodes == num_mappings == num_features, "Non-agreement in number of nodes"
        if sum(num_nodes.values()) == 0 or sum(num_edges.values()) == 0:
            raise ValueError("Zero nodes/edges found")
        # Conversion
        edges_dict = {}
        for edge_type, edge_set in self._edges.items():
            source_nodes, dest_nodes = [], []
            for edge in edge_set:
                source_nodes.append(edge[0])
                dest_nodes.append(edge[1])
            edges_dict[edge_type] = (
                torch.tensor(source_nodes, dtype=torch.int64),
                torch.tensor(dest_nodes, dtype=torch.int64)
            )
        feature_dict = {}
        for node_type, features in self._node_features.items():
            feature_list: List = [None] * len(features)
            for node_id, feature in features.items():
                feature_list[node_id] = feature
            if not feature_list:
                # 0 nodes in particular feature
                feature_list = torch.tensor([])
            else:
                feature_list = torch.stack(feature_list)
            feature_dict[node_type] = feature_list
        dgl_graph = dgl.heterograph(edges_dict)
        dgl_graph.ndata["features"] = feature_dict
        return dgl_graph

    @property
    def mappings(self):
        def get_name(obj: Union[MethodAnalysis, ClassAnalysis, str]) -> str:
            if isinstance(obj, MethodAnalysis):
                return str(obj.full_name)
            elif isinstance(obj, ClassAnalysis):
                return str(obj.name)
            else:
                return str(obj)

        return {
            node_type: {node_id: get_name(node_name) for node_name, node_id in mappings.items()}
            for node_type, mappings in self._node_to_int.items()
        }


def process_apk(dx: Analysis, config: APIConfig) -> Tuple[HeteroGraph, HeteroGraph]:
    """
    Processes input Analysis object both class-wise and method-wise and returns corresponding heterographs
    :param config: APIConfig object
    :param dx: Analysis object returned from AnalyzeAPK
    :return:
    """
    edge_types = [
        ('user', 'invokes', 'user'),
        ('user', 'invokes_api', 'api'),
        ('user', 'parent_of', 'user'),
        ('api', 'api_parent_of', 'user'),
        ('api', 'uses', 'permission'),
        ('api', 'calls_back', 'api')
    ]
    feature_extractors = {
        'user': FeatureExtractors.get_user_features,
        'api': FeatureExtractors.get_api_features,
        'permission': FeatureExtractors.get_permission_features
    }
    fcg = HeteroGraph(
        edge_types=edge_types,
        reverse=True,
        feature_extractors=feature_extractors,
        data_type_checks={
            'user': lambda user: not user.is_external(),
            'api': lambda api: api.is_external(),
            'permission': lambda permission: isinstance(permission, str)
        }
    )
    reduced_fcg = HeteroGraph(
        edge_types=edge_types,
        reverse=True,
        feature_extractors=feature_extractors,
        data_type_checks={
            'user': lambda user: not user.is_external(),
            'api': lambda api: api.is_external(),
            'permission': lambda permission: isinstance(permission, str)
        }
    )
    # Adding inheritance edges
    inheritance_graph = get_inheritance_graph(dx)
    for parent in inheritance_graph:
        if parent == 'Ljava/lang/Object;':
            continue
        parent_class: ClassAnalysis = dx.get_class_analysis(parent)
        if parent_class is None:
            stats["parent==null"] += 1
            continue
        parent_method_names: Dict[str, MethodAnalysis] = {
            f'{x.name} {x.descriptor}': x for x in parent_class.get_methods()
        }
        class_source_type = 'api' if parent_class.is_external() else 'user'
        for child in inheritance_graph[parent]:
            child_class: ClassAnalysis = dx.get_class_analysis(child)
            if child_class is None:
                continue
            child_method_names: Dict[str, MethodAnalysis] = {
                f'{x.name} {x.descriptor}': x for x in child_class.get_methods()
            }
            # Intersection happening here!
            common_method_names = set(parent_method_names) & set(child_method_names)
            class_dest_type = 'api' if child_class.is_external() else 'user'
            if class_dest_type == 'api':
                stats['inheritance: dest==api'] += 1
                continue
            edge_name = 'api_parent_of' if class_source_type == 'api' else 'parent_of'
            edge_type = (class_source_type, edge_name, class_dest_type)
            reduced_fcg.insert(parent_class, child_class, edge_type=edge_type)
            for common_method in common_method_names:
                parent_method = parent_method_names[common_method]
                child_method = child_method_names[common_method]
                if child_method.is_external():
                    stats['inheritance: dest==api'] += 1
                    continue
                source_type = 'api' if parent_method.is_external() else 'user'
                dest_type = 'user'  # Because of if above
                edge_name = 'api_parent_of' if source_type == 'api' else 'parent_of'
                edge_type = (source_type, edge_name, dest_type)
                fcg.insert(parent_method, child_method, edge_type=edge_type)
    # Adding invoke edges
    method: MethodAnalysis
    for method in dx.get_methods():
        class_analysis = dx.get_class_analysis(method.class_name)
        if method.is_external():
            # Get permissions
            permissions = FeatureExtractors.get_used_permissions(method)
            for permission in permissions:
                edge_type = ('api', 'uses', 'permission')
                fcg.insert(method, permission, edge_type=edge_type)
                reduced_fcg.insert(class_analysis, permission, edge_type=edge_type)
            # Add api->api edges
            if str(method.full_name) in config.registration_methods:
                stats["registration_methods"] += 1
                if len(method.get_xref_from()) == 0:
                    stats["skipped registration"] += 1
                    continue
                result_entry = config.result_remapping[str(method.class_name)]
                # method_name will be in result_entry['registration_methods']
                # We shall add now add edges from api->api
                for mapping in result_entry['used->callback mapping']:
                    try:
                        used_method = dx.get_method_analysis_by_name(*mapping[0].split(' '))
                    except TypeError:
                        used_method = None
                    try:
                        callback_method = dx.get_method_analysis_by_name(*mapping[1].split(' '))
                    except TypeError:
                        callback_method = None
                    if not callback_method:
                        stats["callback_method==None"] += 1
                        continue
                    if not used_method:
                        # If used method is not in method list, we shall make registration as used method
                        used_method = method
                    else:
                        stats["used_method!=None"] += 1
                    used_class = dx.get_class_analysis(used_method.class_name)
                    callback_class = dx.get_class_analysis(callback_method.class_name)
                    edge_type = ('api', 'calls_back', 'api')
                    fcg.insert(used_method, callback_method, edge_type=edge_type)
                    reduced_fcg.insert(used_class, callback_class, edge_type=edge_type)
        else:
            called_methods: Set[Tuple[ClassAnalysis, MethodAnalysis, int]] = method.get_xref_to()
            for called_class, called_method, offset in called_methods:
                if called_method.is_external():
                    edge_type = ('user', 'invokes_api', 'api')
                else:
                    edge_type = ('user', 'invokes', 'user')
                fcg.insert(method, called_method, edge_type=edge_type)
                if called_class.is_external():
                    edge_type = ('user', 'invokes_api', 'api')
                else:
                    edge_type = ('user', 'invokes', 'user')
                reduced_fcg.insert(class_analysis, called_class, edge_type=edge_type)
    return fcg, reduced_fcg


def process(config: APIConfig, source_file: Path, dest_dir: Path):
    """
    Process APK file and export call graph in DGL format
    :param config:
    :param source_file: Path of APK file
    :param dest_dir: Path of the directory to store processed graph
    :return: None
    """
    try:
        file_name = source_file.stem
        a, d, dx = AnalyzeAPK(str(source_file))
        fcg, reduced_fcg = process_apk(dx, config)
        dgl.data.utils.save_graphs(str(dest_dir / f"{file_name}.fcg"), [fcg.to_dgl()])
        dgl.data.utils.save_graphs(str(dest_dir / f"{file_name}.rfcg"), [reduced_fcg.to_dgl()])
        json.dump(fcg.mappings, open(dest_dir / f"{file_name}.fcg.map", "w"))
        json.dump(reduced_fcg.mappings, open(dest_dir / f"{file_name}.rfcg.map", "w"))
        json.dump(stats, open(dest_dir / f"{file_name}.rfcg.stat", "w"))
        print(f"Processed {source_file}")
    except:
        e = sys.exc_info()
        with open("err.log", "a") as f:
            f.write(f"Error while processing {source_file}")
            traceback.print_exception(*e, file=f)
        print(f"Error while processing {source_file}")
        traceback.print_exception(*e)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Preprocess APK Dataset into Graphs')
    parser.add_argument(
        '--source-dir',
        help='The directory containing apks',
        required=True
    )
    parser.add_argument(
        '--dest-dir',
        help='The directory to store processed graphs',
        required=True
    )
    parser.add_argument(
        '--metadata-dir',
        help='The directory pointing to metadata. Must contain api_analysis/',
        required=True
    )
    parser.add_argument(
        '--override',
        help='Override existing processed files',
        action='store_true'
    )
    parser.add_argument(
        '--dry',
        help='Run without actual processing',
        action='store_true'
    )
    parser.add_argument(
        '--limit',
        help='Run for n apks',
        default=-1
    )
    args = parser.parse_args()
    source_dir = Path(args.source_dir)
    if not source_dir.exists():
        raise FileNotFoundError(f'{source_dir} not found')
    dest_dir = Path(args.dest_dir)
    if not dest_dir.exists():
        raise FileNotFoundError(f'{dest_dir} not found')
    metadata_dir = Path(args.metadata_dir)
    if not metadata_dir.exists():
        raise FileNotFoundError(f'{metadata_dir} not found')
    api_config = APIConfig(metadata_dir)
    files = [x for x in source_dir.iterdir() if x.is_file()]
    source_files = set([x.stem for x in files])
    dest_files = set([x.stem for x in dest_dir.iterdir() if x.is_file()])
    unprocessed = [source_dir / f'{x}.apk' for x in source_files - dest_files]
    print(f"Only {len(unprocessed)} out of {len(source_files)} remain to be processed")
    if args.override:
        print(f"--override specified. Ignoring {len(source_files) - len(unprocessed)} processed files")
        unprocessed = [source_dir / f'{x}.apk' for x in source_files]
    print(f"Starting dataset processing")
    if args.limit != -1:
        print(f"Limiting dataset processing to {args.limit}")
    if not args.dry:
        J.Parallel(n_jobs=multiprocessing.cpu_count())(J.delayed(process)(api_config, x, dest_dir) for x in unprocessed)
    print("DONE")
