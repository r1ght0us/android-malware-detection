from typing import List, Mapping, Tuple, Set

import torch
import pandas as pd
import dgl
from pgmpy.estimators.CITests import chi_square
from pgmpy.estimators import HillClimbSearch, BicScore, BayesianEstimator
from pgmpy.models.BayesianModel import BayesianModel


class GraphExplainer:
    def __init__(
            self,
            model: torch.nn.Module,
            graph: dgl.DGLHeteroGraph,
            device: str = 'cpu'
    ):
        """
        The GraphExplainer module
        :param model: the GNN to be explained. Should have a forward() function taking a dgl.DGLGraph as argument,
        and returning
        :param graph: the (batched) dgl.DGLGraph instance to generate the explanation for
        :param device: The address of the device to perform computations on.
        """
        self.model = model.to(device)
        self.model.eval()
        self.graph = graph.to(device)
        self.prediction = torch.sigmoid(self.model(self.graph)).detach().item()
        print(f"Model prediction on the graph = {self.prediction}")
        self.features = graph.ndata['features']
        self.device = device
        self.num_nodes = {x: graph.num_nodes(x) for x in self.features.keys()}
        self.total_num_nodes = graph.num_nodes()
        self.node_names = []
        for node_type, num_nodes in self.num_nodes.items():
            self.node_names += [f'{node_type}_{i}' for i in range(num_nodes)]

    def perturb_features(
            self,
            num_samples: int,
            min_diff: float,
            perturb_ratio: float,
            batch_size: int
    ) -> pd.DataFrame:
        """
        Perturbs features on nodes and returns the data table consisting of
        (num_nodes + 1) x num_sample entries
        :param num_samples: Number of times sampling has to be done
        :param min_diff: Minimum difference in the model prediction to be considered as significant
        :param perturb_ratio: Fraction of the nodes to perturb
        :param batch_size: Number of graphs to give input to the model at a single time
        :return: The DataFrame consisting of perturbation indices and prediction change for n_samples
        """
        samples = []
        graphs = []
        perturb_indices = []

        def predict_on_batch():
            nonlocal graphs, perturb_indices
            batch = dgl.batch(graphs)
            prediction = torch.sigmoid(self.model(batch)).detach().to('cpu')
            difference = self.prediction - prediction
            significant_difference = (difference > min_diff).reshape(-1, 1)
            samples.append(
                torch.cat(
                    (
                        torch.stack(perturb_indices),
                        significant_difference
                    ),
                    dim=1
                )
            )
            graphs.clear()
            perturb_indices.clear()

        for i in range(num_samples):
            perturbed_graph = self.graph.clone()
            perturbed_features = {
                node_type: feature.clone().to(self.device)
                for node_type, feature in self.features.items()
            }
            perturb_node_idx = torch.rand(self.total_num_nodes) < perturb_ratio
            start = 0
            for node_type, num_nodes in self.num_nodes.items():
                perturb_node_indices = perturb_node_idx[start:start + num_nodes]
                perturbed_features[node_type][perturb_node_indices] = 0
                start += num_nodes
            perturb_indices.append(perturb_node_idx)
            perturbed_graph.ndata['features'] = perturbed_features
            graphs.append(perturbed_graph)
            if (i + 1) % batch_size == 0:
                predict_on_batch()
        if len(graphs) > 0:
            predict_on_batch()
        data = pd.DataFrame(torch.vstack(samples).int().numpy())
        data.columns = self.node_names + ['t']
        return data

    def select_nodes(
            self,
            data: pd.DataFrame,
            num_nodes: int
    ) -> List[Tuple[str, float]]:
        """
        Selects crucial nodes required for target t
        :param data: A pandas dataframe consisting of samples x num_nodes + 1 columns,
         where last column is labelled as 't'
        :param num_nodes: Number of nodes to select
        :return: List of tuples where each tuple consists of node label and its rank
        """
        result = []
        for node in self.node_names:
            chi, dof, p = chi_square(
                X=node,
                Y='t',
                Z=[],
                data=data,
                boolean=False
            )
            result.append((node, p))
        result = sorted(result, key=lambda x: x[1])
        return result[:num_nodes]

    @staticmethod
    def get_markov_blanket(
            nodes: List[int],
            data: pd.DataFrame,
            min_p: float
    ):
        """
        Returns the Markov Blanket of nodes using data
        :param data: A pandas dataframe consisting of samples x num_nodes + 1 columns,
         where last column is labelled as 't'
        :param nodes: A subset of nodes of the groups which are highly dependent on 't'
        :param min_p: Minimum p-value while checking for independence
        :return: Set of nodes in Markov Blanket of 't'
        """
        MB = set(nodes)  # Initially set markov blanket to every nodes
        i = 0
        while True:
            changed = False
            print(f"Loop {i}. MB consists of {len(MB)} nodes")
            i += 1
            for i, node in enumerate(nodes):
                print(f"{i}/{len(nodes)}", end='\r')
                if node not in MB:
                    continue
                # Check 't' _|_ MB - {node} | {node}
                chi2, dof, p = chi_square(
                    X='t',
                    Y=node,
                    Z=MB - {node},
                    data=data,
                    boolean=False
                )
                if p > min_p:
                    changed = True
                    MB = MB - {node}
            if not changed:
                return MB

    @staticmethod
    def get_markov_blanket1(
            nodes: List[int],
            data: pd.DataFrame,
            min_p: float
    ):
        """
        Returns the Markov Blanket of nodes using data
        :param data: A pandas dataframe consisting of samples x num_nodes + 1 columns,
         where last column is labelled as 't'
        :param nodes: A subset of nodes of the groups which are highly dependent on 't'
        :param min_p: Minimum p-value while checking for independence
        :return: Set of nodes in Markov Blanket of 't'
        """
        MB = set(nodes)
        while True:
            count = 0
            for node in nodes:
                evidences = MB.copy()
                evidences -= {node}
                chi, dof, p = chi_square(
                    X='t',
                    Y=node,
                    Z=evidences,
                    data=data,
                    boolean=False
                )
                if p > min_p:
                    MB -= {node}
                    count = 0
                else:
                    count = count + 1
                    if count == len(MB):
                        return MB

    @staticmethod
    def fit_pgm(
            data: pd.DataFrame,
            markov_blanket: Set[str]
    ) -> BayesianModel:
        """
        Get the explanation as a PGM
        :param data: A pandas dataframe consisting of samples x num_nodes + 1 columns,
         where last column is labelled as 't'
        :param markov_blanket: Set of nodes in Markov Blanket of 't'
        :return: A Bayesian model of the explanation
        """
        data_subset = data[markov_blanket].copy()
        estimator = HillClimbSearch(
            data=data_subset,
            scoring_method=BicScore(data_subset)
        )
        pgm = estimator.estimate()
        for node in markov_blanket:
            pgm.add_edge(node, 't')
        explanation = BayesianModel()
        for node in pgm.nodes():
            explanation.add_node(node)
        for edge in pgm.edges():
            explanation.add_edge(edge[0], edge[1])
        # Now fit the explanation
        data_subset['t'] = data['t']
        explanation.fit(
            data_subset,
            estimator=BayesianEstimator,
            prior_type='K2'
        )
        return explanation

    def explain(
            self,
            num_samples: int = 5000,
            min_diff: float = 0.01,
            perturb_ratio: float = 0.5,
            num_nodes: int = 20,
            batch_size: int = 64,
            min_p: float = 0.05
    ) -> BayesianModel:
        """
        Explains the prediction of the given model on the given graph
        :param num_samples: Number of times sampling has to be done
        :param min_diff: Minimum difference in the model prediction to be considered as significant
        :param perturb_ratio: Fraction of the nodes to perturb
        :param batch_size: Number of graphs to give input to the model at a single time
        :param num_nodes: Number of nodes to consider while building PGM
        :param min_p: Minimum p-value while checking for independence in Markov Blanket generation
        :return: A Bayesian Model corresponding to the explanation with -1 as the target node
        """
        print(f"Starting explanation with {num_samples} samples\n"
              f"Total number of nodes is {self.graph.num_nodes()}\n"
              f"{perturb_ratio * 100}% nodes' features will be perturbed\n"
              f"A perturbation results in significant change only if\n"
              f"there is >={min_diff * 100}% change in model's output\n"
              f"The model is given input in the batches of size {batch_size}\n"
              f"The resulting PGM will consist of {num_nodes}+1 nodes,\n"
              f"With additional node labelled -1 representing `t`")
        print("-" * 50)
        print("Starting sampling")
        data = self.perturb_features(
            num_samples=num_samples,
            min_diff=min_diff,
            perturb_ratio=perturb_ratio,
            batch_size=batch_size
        )
        print("Sampling done")
        print(data)
        print(data['t'].value_counts())
        print(f"Selecting top {num_nodes} nodes")
        top_nodes = self.select_nodes(data, num_nodes)
        print("Top nodes selected", top_nodes)
        nodes = [x[0] for x in top_nodes]
        print("Generating Markov blanket")
        markov_blanket = self.get_markov_blanket(nodes, data, min_p)
        print(f"Markov blanket consists of {len(markov_blanket)} nodes")
        print("Start fitting PGM for explanation")
        return self.fit_pgm(data, markov_blanket)
